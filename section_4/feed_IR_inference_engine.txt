Earlier in the course, you were focused on working with the Intermediate Representation (IR) models themselves, while mostly glossing over the use of the actual Inference Engine with the model.

Here, you'll import the Python wrapper for the Inference Engine (IE), and practice using different IRs with it. You will first add each IR as an IENetwork, and check whether the layers of that network are supported by the classroom CPU.

Since the classroom workspace is using an Intel CPU, you will also need to add a CPU extension to the IECore.

Once you have verified all layers are supported (when the CPU extension is added), you will load the given model into the Inference Engine.

Note that the .xml file of the IR should be given as an argument when running the script.

To test your implementation, you should be able to successfully load each of the three IR model files we have been working with throughout the course so far, which you can find in the /home/workspace/models directory.

Solution:

First, add the additional libraries (os may not be needed depending on how you get the model file names):

### Load the necessary libraries
import os
from openvino.inference_engine import IENetwork, IECore
Then, to load the Intermediate Representation and feed it to the Inference Engine:

def load_to_IE(model_xml):
    ### Load the Inference Engine API
    plugin = IECore()

    ### Load IR files into their related class
    model_bin = os.path.splitext(model_xml)[0] + ".bin"
    net = IENetwork(model=model_xml, weights=model_bin)

    ### Add a CPU extension, if applicable.
    plugin.add_extension(CPU_EXTENSION, "CPU")

    ### Get the supported layers of the network
    supported_layers = plugin.query_network(network=net, device_name="CPU")

    ### Check for any unsupported layers, and let the user
    ### know if anything is missing. Exit the program, if so.
    unsupported_layers = [l for l in net.layers.keys() if l not in supported_layers]
    if len(unsupported_layers) != 0:
        print("Unsupported layers found: {}".format(unsupported_layers))
        print("Check whether extensions are available to add to IECore.")
        exit(1)

    ### Load the network into the Inference Engine
    plugin.load_network(net, "CPU")

    print("IR successfully loaded into Inference Engine.")

    return
Note that a more optimal approach here would actually check whether a CPU extension was added as an argument by the user, but to keep things simple, I hard-coded it for the exercise.

Running Your Implementation
You should make sure your implementation runs with all three pre-trained models we worked with earlier (and you are welcome to also try the models you converted in the previous lesson from TensorFlow, Caffe and ONNX, although your workspace may not have these stored). I placed these in the /home/workspace/models directory for easier use, and because the workspace will reset the /opt directory between sessions.

python feed_network.py -m /home/workspace/models/human-pose-estimation-0001.xml
You can run the other two by updating the model name in the above.