You've come a long way from the first lesson where most of the code for working with the OpenVINO toolkit was happening in the background. You worked with pre-trained models, moved up to converting any trained model to an Intermediate Representation with the Model Optimizer, and even got the model loaded into the Inference Engine and began making inference requests.

In this final exercise of this lesson, you'll close off the OpenVINO workflow by extracting the results of the inference request, and then integrating the Inference Engine into an existing application. You'll still be given some of the overall application infrastructure, as more that of will come in the next lesson, but all of that is outside of OpenVINO itself.

You will also add code allowing you to try out various confidence thresholds with the model, as well as changing the visual look of the output, like bounding box colors.

Now, it's up to you which exact model you want to use here, although you are able to just re-use the model you converted with TensorFlow before for an easy bounding box dectector.

Note that this application will run with a video instead of just images like we've done before.

So, your tasks are to:

Convert a bounding box model to an IR with the Model Optimizer.
Pre-process the model as necessary.
Use an async request to perform inference on each video frame.
Extract the results from the inference request.
Add code to make the requests and feed back the results within the application.
Perform any necessary post-processing steps to get the bounding boxes.
Add a command line argument to allow for different confidence thresholds for the model.
Add a command line argument to allow for different bounding box colors for the output.
Correctly utilize the command line arguments in #3 and #4 within the application.
When you are done, feed your model to app.py, and it will generate out.mp4, which you can download and view. Note that this app will take a little bit longer to run. Also, if you need to re-run inference, delete the out.mp4 file first.

You only need to feed the model with -m before adding the customization; you should set defaults for any additional arguments you add for the color and confidence so that the user does not always need to specify them.


python app.py -m {your-model-path.xml}

Solution:

Note: There is one small change from the code on-screen for running on Linux machines versus Mac. On Mac, cv2.VideoWriter uses cv2.VideoWriter_fourcc('M','J','P','G') to write an .mp4 file, while Linux uses 0x00000021.

Functions in inference.py
I covered the async and wait functions here as it's split out slightly differently than we saw in the last exercise.

First, it's important to note that output and input blobs were grabbed higher above when the network model is loaded:

self.input_blob = next(iter(self.network.inputs))
self.output_blob = next(iter(self.network.outputs))
From there, you can mostly use similar code to before:

    def async_inference(self, image):
        '''
        Makes an asynchronous inference request, given an input image.
        '''
        self.exec_network.start_async(request_id=0, 
            inputs={self.input_blob: image})
        return


    def wait(self):
        '''
        Checks the status of the inference request.
        '''
        status = self.exec_network.requests[0].wait(-1)
        return status
You can grab the network output using the appropriate request with the output_blob key:

    def extract_output(self):
        '''
        Returns a list of the results for the output layer of the network.
        '''
        return self.exec_network.requests[0].outputs[self.output_blob]
app.py
The next steps in app.py, before customization, are largely based on using the functions in inference.py:

    ### Initialize the Inference Engine
    plugin = Network()

    ### Load the network model into the IE
    plugin.load_model(args.m, args.d, CPU_EXTENSION)
    net_input_shape = plugin.get_input_shape()

    ...

        ### Pre-process the frame
        p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))
        p_frame = p_frame.transpose((2,0,1))
        p_frame = p_frame.reshape(1, *p_frame.shape)

        ### Perform inference on the frame
        plugin.async_inference(p_frame)

        ### Get the output of inference
        if plugin.wait() == 0:
            result = plugin.extract_output()
            ### Update the frame to include detected bounding boxes
            frame = draw_boxes(frame, result, args, width, height)
            # Write out the frame
            out.write(frame)
The draw_boxes function is used to extract the bounding boxes and draw them back onto the input image.

def draw_boxes(frame, result, args, width, height):
    '''
    Draw bounding boxes onto the frame.
    '''
    for box in result[0][0]: # Output shape is 1x1x100x7
        conf = box[2]
        if conf >= 0.5:
            xmin = int(box[3] * width)
            ymin = int(box[4] * height)
            xmax = int(box[5] * width)
            ymax = int(box[6] * height)
            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 0, 255), 1)
    return frame
Customizing app.py
Adding the customization only took a few extra steps.

Parsing the command line arguments
First, you need to add the additional command line arguments:

    c_desc = "The color of the bounding boxes to draw; RED, GREEN or BLUE"
    ct_desc = "The confidence threshold to use with the bounding boxes"

    ...

    optional.add_argument("-c", help=c_desc, default='BLUE')
    optional.add_argument("-ct", help=ct_desc, default=0.5)
The names and descriptions here, and even how you use the default values, can be up to you.

Handle the new arguments
I needed to also process these arguments a little further. This is pretty open based on your own implementation - since I took in a color string, I need to convert it to a BGR tuple for use as a OpenCV colors.

def convert_color(color_string):
    '''
    Get the BGR value of the desired bounding box color.
    Defaults to Blue if an invalid color is given.
    '''
    colors = {"BLUE": (255,0,0), "GREEN": (0,255,0), "RED": (0,0,255)}
    out_color = colors.get(color_string)
    if out_color:
        return out_color
    else:
        return colors['BLUE']
I then need to call this with the related argument, as well as make sure the confidence threshold argument is a float value.

    args.c = convert_color(args.c)
    args.ct = float(args.ct)
Adding customization to draw_boxes()
The final step was to integrate these new arguments into my draw_boxes() function. I needed to make sure that the arguments are fed to the function:

frame = draw_boxes(frame, result, args, width, height)
and then I can use them where appropriate in the updated function.

def draw_boxes(frame, result, args, width, height):
    '''
    Draw bounding boxes onto the frame.
    '''
    for box in result[0][0]: # Output shape is 1x1x100x7
        conf = box[2]
        if conf >= args.ct:
            xmin = int(box[3] * width)
            ymin = int(box[4] * height)
            xmax = int(box[5] * width)
            ymax = int(box[6] * height)
            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), args.c, 1)
    return frame
With everything implemented, I could run my app as such (given I re-used the previously converted TF model from the Model Optimizer lesson) if I wanted blue bounding boxes and a confidence threshold of 0.6:

python app.py -m frozen_inference_graph.xml -ct 0.6 -c BLUE