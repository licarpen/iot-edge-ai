In the previous exercise, you loaded Intermediate Representations (IRs) into the Inference Engine. Now that we've covered some of the topics around requests, including the difference between synchronous and asynchronous requests, you'll add additional code to make inference requests to the Inference Engine.

Given an ExecutableNetwork that is the IR loaded into the Inference Engine, your task is to:

Perform a synchronous request
Start an asynchronous request given an input image frame
Wait for the asynchronous request to complete
Note that we'll cover handling the results of the request shortly, so you don't need to worry about that just yet. This will get you practice with both types of requests with the Inference Engine.

You will perform the above tasks within inference.py. This will take three arguments, one for the model, one for the test image, and the last for what type of inference request should be made.

You can use test.py afterward to verify your code successfully makes inference requests.

Solution:

Synchronous Solution
def sync_inference(exec_net, input_blob, image):
    '''
    Performs synchronous inference
    Return the result of inference
    '''
    result = exec_net.infer({input_blob: image})

    return result
Asynchronous Solution
def async_inference(exec_net, input_blob, image):
    '''
    Performs asynchronous inference
    Returns the `exec_net`
    '''
    exec_net.start_async(request_id=0, inputs={input_blob: image})
    while True:
        status = exec_net.requests[0].wait(-1)
        if status == 0:
            break
        else:
            time.sleep(1)
    return exec_net
I don't actually need time.sleep() here - using the -1 with wait() is able to perform similar functionality.

Testing
You can run the test file to check your implementations using inference on multiple models.

python test.py


