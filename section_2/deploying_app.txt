


So far, you've downloaded some pre-trained models, handled their inputs, and learned how to handle outputs. In this exercise, you'll implement the handling of the outputs of our three models from before, and get to see inference actually performed by adding these models to some example edge applications.

There's a lot of code still involved behind the scenes here. With the Pre-Trained Models available with the OpenVINO toolkit, you don't need to worry about the Model Optimizer, but there is still work done to load the model into the Inference Engine. We won't learn about this code until later, so in this case, you'll just need to call your functions to handle the input and output of the model within the app.

If you do want a sneak preview of some of the code that interfaces with the Inference Engine, you can check it out in inference.py. You'll work out of the handle_models.py file, as well as adding functions calls within the edge app in app.py.

TODOs
In handle_models.py, you will need to implement handle_pose, handle_text, and handle_car.

In app.py, first, you'll need to use the input shape of the network to call the preprocessing function. Then, you need to call handle_output with the appropriate model argument in order to get the right handling function. With that function, you can then feed the output of the inference request in in order to extract the output.

Note that there is some additional post-processing done for you in create_output_image within app.py to help display the output back onto the input image.

Testing the apps
To test your implementations, you can use app.py to run each edge application, with the following arguments:

-t: The model type, which should be one of "POSE", "TEXT", or "CAR_META"
-m: The location of the model .xml file
-i: The location of the input image used for testing
-c: A CPU extension file, if applicable. See below for what this is for the workspace. The results of your output will be saved down for viewing in the outputs directory.
As an example, here is an example of running the app with related arguments:


python app.py -i "images/blue-car.jpg" -t "CAR_META" -m "/home/workspace/models/vehicle-attributes-recognition-barrier-0039.xml" -c "/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so"
Model Documentation
Once again, here are the links to the models, so you can use the Output section to help you get started (there are additional comments in the code to assist):

Human Pose Estimation: human-pose-estimation-0001
Text Detection: text-detection-0004
Determining Car Type & Color: vehicle-attributes-recognition-barrier-0039

Solution:
The code for calling preprocessing and utilizing the output handling functions from within app.py is fairly straightforward:

preprocessed_image = preprocessing(image, h, w)
This is just feeding in the input image, along with height and width of the network, which the given inference_network.load_model function actually returned for you.

output_func = handle_output(args.t)
processed_output = output_func(output, image.shape)
This is partly based on the helper function I gave you, which can return the correct output handling function by feeding in the model type. The second line actually sends the output of inference and image shape to whichever output handling function is appropriate.

Car Meta Output Handling
Given that the two outputs for the Car Meta Model are "type" and "color", and are just the softmax probabilities by class, I wanted you to just return the np.argmax, or the index where the highest probability was determined.

def handle_car(output, input_shape):
    '''
    Handles the output of the Car Metadata model.
    Returns two integers: the argmax of each softmax output.
    The first is for color, and the second for type.
    '''
    # Get rid of unnecessary dimensions
    color = output['color'].flatten()
    car_type = output['type'].flatten()
    # TODO 1: Get the argmax of the "color" output
    color_pred = np.argmax(color)
    # TODO 2: Get the argmax of the "type" output
    type_pred = np.argmax(car_type)

    return color_pred, type_pred
Run the Car Meta Model
I have moved the models used in the exercise into a models subdirectory in the /home/workspace directory, so the path used can be a little bit shorter.

python app.py -i "images/blue-car.jpg" -t "CAR_META" -m "/home/workspace/models/vehicle-attributes-recognition-barrier-0039.xml" -c "/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so"
For the other models, make sure to update the input image -i, model type -t, and model -m accordingly.

Pose Estimation Output Handling
Handling the car output was fairly straightforward by using np.argmax, but the outputs for the pose estimation and text detection models is a bit trickier. However, there's a lot of similar code between the two. In this second part of the solution, I'll go into detail on the pose estimation model, and then we'll finish with a quick video on handling the output of the text detection model.


Pose Estimation is more difficult, and doesn't have as nicely named outputs. I noted you just need the second one in this exercise, called 'Mconv7_stage2_L2', which is just the keypoint heatmaps, and not the associations between these keypoints. From there, I created an empty array to hold the output heatmaps once they are re-sized, as I decided to iterate through each heatmap 1 by 1 and re-size it, which can't be done in place on the original output.

def handle_pose(output, input_shape):
    '''
    Handles the output of the Pose Estimation model.
    Returns ONLY the keypoint heatmaps, and not the Part Affinity Fields.
    '''
    # TODO 1: Extract only the second blob output (keypoint heatmaps)
    heatmaps = output['Mconv7_stage2_L2']
    # TODO 2: Resize the heatmap back to the size of the input
    # Create an empty array to handle the output map
    out_heatmap = np.zeros([heatmaps.shape[1], input_shape[0], input_shape[1]])
    # Iterate through and re-size each heatmap
    for h in range(len(heatmaps[0])):
        out_heatmap[h] = cv2.resize(heatmaps[0][h], input_shape[0:2][::-1])

    return out_heatmap
Note that the input_shape[0:2][::-1] line is taking the original image shape of HxWxC, taking just the first two (HxW), and reversing them to be WxH as cv2.resize uses.

Text Detection Model Handling
Thanks for sticking in there! The code for the text detection model is pretty similar to the pose estimation one, so let's finish things off.


Text Detection had a very similar output processing function, just using the 'model/segm_logits/add' output and only needing to resize over two "channels" of output. I likely could have extracted this out into its own output handling function that both Pose Estimation and Text Detection could have used.

def handle_text(output, input_shape):
    '''
    Handles the output of the Text Detection model.
    Returns ONLY the text/no text classification of each pixel,
        and not the linkage between pixels and their neighbors.
    '''
    # TODO 1: Extract only the first blob output (text/no text classification)
    text_classes = output['model/segm_logits/add']
    # TODO 2: Resize this output back to the size of the input
    out_text = np.empty([text_classes.shape[1], input_shape[0], input_shape[1]])
    for t in range(len(text_classes[0])):
        out_text[t] = cv2.resize(text_classes[0][t], input_shape[0:2][::-1])

    return out_text